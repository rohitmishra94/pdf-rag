{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e252ce87-bf67-461f-8fd6-26ee1c0d0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env variable loaded:  True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89c33356d8b43e495d010894273238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from FlagEmbedding import FlagReranker\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import pymupdf4llm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging.config\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "print('env variable loaded: ',load_dotenv('/workspace/test/pdf-rag/env'))\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "AsyncClient = AsyncOpenAI()\n",
    "\n",
    "def embedding_function_bge(text_list):\n",
    "    return model.encode(text_list, return_dense=True)['dense_vecs']\n",
    "\n",
    "\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embeddings = embedding_function_bge(input)\n",
    "        return embeddings\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n",
    "default_ef = MyEmbeddingFunction()\n",
    "client = chromadb.PersistentClient(path=\"chromadb_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4c378f-238c-4d96-9943-eb1b7a4a7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_function_bge(['iam there'])\n",
    "# default_ef(['iam there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af0c5aa-c635-4c6e-b9f5-41adfe6cc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(texts, chunk_size=100, overlap=30):\n",
    "    \"\"\"Process a list of texts, splitting them into chunks of specified size with overlap, \n",
    "    and accumulating shorter texts.\"\"\"\n",
    "    accumulated_words = []  # Accumulate words from texts shorter than chunk_size\n",
    "    final_chunks = []  # Store the final chunks of text\n",
    "\n",
    "    for text in texts.split():\n",
    "        accumulated_words.append(text)\n",
    "\n",
    "        while len(accumulated_words) >= chunk_size:\n",
    "            # Take the first chunk_size words for the current chunk\n",
    "            chunk = \" \".join(accumulated_words[:chunk_size])\n",
    "            final_chunks.append(chunk)\n",
    "            # Remove words from the start of the accumulated_words, considering overlap\n",
    "            accumulated_words = accumulated_words[chunk_size - overlap:]\n",
    "    \n",
    "    # If there are any remaining words, form the last chunk\n",
    "    if accumulated_words:\n",
    "        final_chunks.append(\" \".join(accumulated_words))\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "def get_unique_text_indices(text_list):\n",
    "    unique_texts = {}\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        if text not in unique_texts:\n",
    "            unique_texts[text] = i\n",
    "            unique_indices.append(i)\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "\n",
    "    try:\n",
    "        # Get the tokenizer for the specified model\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Default to a generic encoding if the model is unknown\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Tokenize the text and return the token count\n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c974a8-4111-48e4-b135-27d190c862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf_collection(pdf_path):\n",
    "    \"\"\"\n",
    "    Process a PDF file and add its chunks to a collection.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        collection: The collection object to add documents to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md_text = pymupdf4llm.to_markdown(pdf_path,show_progress=True)\n",
    "        all_chunks = process_texts(md_text, chunk_size=500, overlap=50)\n",
    "\n",
    "        collection_name = pdf_path.split('/')[-1]\n",
    "        collection = client.get_or_create_collection(name=collection_name,embedding_function=default_ef)\n",
    "        logger.info(collection)\n",
    "    \n",
    "        for idx, chunk in tqdm(enumerate(all_chunks)):\n",
    "            id_ = str(idx)\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                ids=[id_]\n",
    "            )\n",
    "        status = 'success'\n",
    "        return status,collection_name\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating pdf collection: {str(e)}\", exc_info=True)\n",
    "        return f'Sorry for inconvenience. Error creating pdf collection. Please contact support.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37496f43-8e0f-4099-89f3-c5488f8a77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_pdf_collection('/workspace/test/pdf-rag/handbook.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcd45f0-aca3-436d-86d9-1496a396900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collections():\n",
    "    return client.list_collections()\n",
    "\n",
    "def load_collection(collection_name):\n",
    "    try:\n",
    "        collection = client.get_or_create_collection(name=collection_name,embedding_function=default_ef)\n",
    "        status = 'success'\n",
    "        return status, collection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Collection load request failed: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91099edd-339c-4e07-8ff2-38c6c6e67a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32fdb6b7-a223-4fe7-b35d-236fa90360d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_context(query, collection, n_results=5, top=2):\n",
    "    logger.info(f'quering collection---> {collection}')\n",
    "    \n",
    "    result = collection.query(query_texts = query,n_results=n_results)\n",
    "    texts = result['documents'][0]\n",
    "    ids = result['ids'][0]\n",
    "    unique_indices = get_unique_text_indices(texts)\n",
    "    unique_docs = [texts[x] for x in unique_indices]\n",
    "    unique_ids = [ids[x] for x in unique_indices]\n",
    "    ## colbert\n",
    "    query_col = model.encode([query],return_colbert_vecs=True)\n",
    "    docs_col = model.encode(unique_docs,return_colbert_vecs=True)\n",
    "    colber_scores = []\n",
    "    for vectors in docs_col['colbert_vecs']:\n",
    "        colber_scores.append(model.colbert_score(query_col['colbert_vecs'][0],vectors).numpy())\n",
    "    \n",
    "    ## full_context_colbert\n",
    "    full_context_scores = []\n",
    "    full_context_ids = []\n",
    "    for id in unique_ids:\n",
    "        pre_id,post_id = str(int(id)-1), str(int(id)+1)\n",
    "        # print(pre_id,id,post_id)\n",
    "        full_context_ids.append([pre_id,id,post_id])\n",
    "        full_context=collection.get(ids=[f'{pre_id}',f'{id}',f'{post_id}'])['documents']\n",
    "        full_context = ''.join(full_context)\n",
    "        full_context_colber_vec = model.encode([full_context],return_colbert_vecs=True)\n",
    "        full_context_colber_score = model.colbert_score(query_col['colbert_vecs'][0],full_context_colber_vec['colbert_vecs'][0]).numpy()\n",
    "    \n",
    "        full_context_scores.append(full_context_colber_score)\n",
    "    \n",
    "    all_scores = [2*full_context_scores[i]+0.9*colber_scores[i] for i in range(len(colber_scores))]\n",
    "    sorted_indices = [index for index, _ in sorted(enumerate(all_scores), key=lambda x: x[1], reverse=True)]\n",
    "    top_context_ids_list = [full_context_ids[index] for index in sorted_indices][:top]\n",
    "    flattened_list = np.array(top_context_ids_list).flatten().tolist()\n",
    "    top_ids = list(set(flattened_list))\n",
    "    top_context = collection.get(ids=top_ids)['documents']\n",
    "\n",
    "    logger.info(f'context retrieved from collection---> {collection}')\n",
    "    return top_context, top_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce951b6-8b7f-4ebc-b1e8-a2cde0e17534",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_response(params: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Generate response using OpenAI API with error handling and logging.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Generating response with model: {params.get('model')}\")\n",
    "        response = await AsyncClient.chat.completions.create(**params)\n",
    "        logger.info(\"Response generated successfully\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating response: {str(e)}\", exc_info=True)\n",
    "        return f'Sorry for inconvenience. Please contact support.'\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "async def chat_completion_request(messages: List[Dict], model='gpt-4o-mini') -> Any:\n",
    "    \"\"\"Make a chat completion request with retry logic.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            'messages': messages,\n",
    "            'max_tokens': 1000,\n",
    "            'model': model,\n",
    "            'temperature': 0,\n",
    "            'response_format': {\"type\": \"json_object\"}\n",
    "        }\n",
    "\n",
    "        response = await generate_response(params)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat completion request failed: {str(e)}\", exc_info=True)\n",
    "        return f'Sorry for inconvenience. Please contact support.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd3f10e-9c88-465e-b57e-5bcbc4ef9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_answer(query,collection_name):\n",
    "    # context = get_context(query,n_results=5,top_results=2,threshold = 0.5)\n",
    "    collection = client.get_or_create_collection(name=collection_name,embedding_function=default_ef)\n",
    "\n",
    "    context, context_idx = get_full_context(query, collection, n_results=5, top=2)\n",
    "    user_query = f'Based on below CONTEXT {context} ANSWER the query {query}'\n",
    "\n",
    "    system_instruction = '''Ideal Output Format\n",
    "The output should be a structured JSON blob that question with its corresponding answer.\n",
    "Answers should be word to word match if the question is a word to word match\n",
    "If the CONTEXT is insufficient, reply with â€œData Not Available'''\n",
    "    \n",
    "    msg = [{\"role\": \"system\", \"content\": system_instruction},{\"role\": \"user\", \"content\": user_query}]\n",
    "    \n",
    "    response = await chat_completion_request(msg)\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    output = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    return output,total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0a49604-3f7b-4111-8379-dd248c3822cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "async def chat_request(messages: List[Dict], tools=None, model='gpt-4o-mini', stream=False) -> Any:\n",
    "    \"\"\"Make a chat completion request with retry logic.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            'messages': messages,\n",
    "            'max_tokens': 1500,\n",
    "            'model': model,\n",
    "            'temperature': 0,\n",
    "            'tools': tools,\n",
    "            'tool_choice': \"auto\",\n",
    "            'stream': stream,\n",
    "        }\n",
    "        response = await generate_response(params)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Chat completion request failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_answer\",\n",
    "            \"description\": \"Use this function to get answers based on context documents from database to user questions. The documents are purely based on user db.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"User query in string.\",\n",
    "                    },\n",
    "                    \"collection\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"db collection name in string.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\",\"collection\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"create_pdf_collection\",\n",
    "            \"description\": \"Use this function to create database collection from pdf file path\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"pdf_path\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"pdf_path in string.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"pdf_path\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_collections\",\n",
    "            \"description\": \"Use this function to check exsiting collection in database\",\n",
    "            \n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = '''\n",
    "Your task is to answer user questions. The user can ask a single question or multiple questions.\n",
    "You have access to the below tools to return answers based on user single/multiple questions.\n",
    "\n",
    "# Tool\n",
    "\"name\": \"get_collections\",\n",
    "\"description\": \"Use this function to check exsiting collection in database\",\n",
    "\"name\": \"create_pdf_collection\",\n",
    "\"description\": \"Use this function to create database collection from pdf file path\",\n",
    "\"name\": \"get_answer\",\n",
    "\"description\": \"Use this function to get answers based on context documents from database to user questions. The documents are purely based on user db.\",\n",
    "                     \n",
    "# Flow of User interaction\n",
    "greet the user in first interaction.\n",
    "and introduce yourself --> you can only provide answers to question based on collection exists in database and list the collections name\n",
    "using get_collection tool\n",
    "\n",
    "if user collection does not exist ask user to provide pdf path and create collection based on pdf path using tool create_pdf_collection\n",
    "pdf path name will be collection name to use. collection creation takes few seconds to ask user to wait few seconds.\n",
    "\n",
    "if user provide collection name , that collection name will be used to answer user query using get_answer tool.\n",
    "you can call get_answer tool multiple time for multiple questions\n",
    "\n",
    "if user chat_history available then collection name based on latest chats will be used to answer query\n",
    "\n",
    "YOUR ONLY TASK TO ANSWER USER QUERY BASED ON COLLECTION AVAILABLE ON DATABASE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "924fa7a0-421c-4e77-bd03-34c656824ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = 'hi'\n",
    "# chat_history = []\n",
    "# system_msg = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "# user_msg = [{\"role\": \"user\", \"content\": user_query}]\n",
    "# feed_msg = system_msg + user_msg\n",
    "# chat_history += feed_msg \n",
    "# chat_answer = await chat_request(feed_msg, tools=tools, stream=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da9b4b14-6f9b-4be0-9ceb-f1d0109e23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b00cbcf-da6f-463a-83a3-18c22eeb0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_msg = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "# chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81eb3d-592e-4b73-962f-bd9df634a1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b5caf43-7143-4365-aa32-d9080ba9fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async def chat_bot(chat_history):\n",
    "    \n",
    "\n",
    "    chat_answer = await chat_request(chat_history, tools=tools, stream=False)\n",
    "    \n",
    "    output_json = {}\n",
    "    if hasattr(chat_answer, 'choices') and chat_answer.choices:\n",
    "        message = chat_answer.choices[0].message\n",
    "        if hasattr(message, 'content') and message.content:\n",
    "            print(f\"\\nResponse: {message.content}\")\n",
    "            assistant_msg = [{\"role\": \"assistant\", \"content\": message.content}]\n",
    "            chat_history += assistant_msg\n",
    "            \n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            for call in message.tool_calls:\n",
    "                \n",
    "                if call.function.name == 'get_answer':  # Corrected dot notation\n",
    "\n",
    "                    print('searching answers...')\n",
    "                    arguments = json.loads(call.function.arguments)\n",
    "                    query = arguments['query']\n",
    "                    collection_name = arguments['collection']\n",
    "                    ans,_ = await get_answer(query,collection_name)\n",
    "                    output_json[query] = ans['answer']\n",
    "                \n",
    "                    \n",
    "                if call.function.name == 'get_collections':\n",
    "\n",
    "                    print('fetching collection list...')\n",
    "                    collection_list = get_collections()\n",
    "                    if collection_list:\n",
    "                        assistant_msg = [{\"role\": \"assistant\", \"content\": f'database collection list {collection_list}'}]\n",
    "                    else: \n",
    "                        assistant_msg = [{\"role\": \"assistant\", \"content\": f'database collection list is empty'}]\n",
    "                    chat_history += assistant_msg\n",
    "                    chat_answer = await chat_bot(chat_history)\n",
    "    \n",
    "                if call.function.name == 'create_pdf_collection':\n",
    "\n",
    "                    print('creating collection...')\n",
    "                    arguments = json.loads(call.function.arguments)\n",
    "                    path = arguments['pdf_path']\n",
    "                    status,collection_name = create_pdf_collection(path)\n",
    "                    if status=='success':\n",
    "                        assistant_msg = [{\"role\": \"assistant\", \"content\": f'database collection created with collection name {collection_name}'}]\n",
    "                    else:\n",
    "                        assistant_msg = [{\"role\": \"assistant\", \"content\": f'database collection creation failed contact support'}]\n",
    "                    chat_history += assistant_msg\n",
    "                    chat_answer = await chat_bot(chat_history)\n",
    "    \n",
    "            \n",
    "            if output_json:\n",
    "                print(\"\\nGenerated JSON Output:\")\n",
    "                print(json.dumps(output_json, indent=2))\n",
    "                assistant_msg = [{\"role\": \"assistant\", \"content\": json.dumps(output_json, indent=2)}]\n",
    "                chat_history += assistant_msg\n",
    "                # chat_answer = await chat_bot(chat_history)\n",
    "            # assistant_msg\n",
    "    \n",
    "    return chat_history\n",
    "        \n",
    "\n",
    "# out = await chat_bot(chat_history)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95a305-2101-4f12-8741-f12024dfd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the name of the company?\" \"Who is the CEO of the company?\" \"What is their vacation policy?\" \"What is the termination policy?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98ae3c10-7b71-4636-a2fd-0e8638aa20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "your input:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response: Hello! I'm here to help you with your questions. I can provide answers based on specific collections in our database. Let me check the existing collections for you. Please hold on for a moment.\n",
      "fetching collection list...\n",
      "\n",
      "Response: I found an existing collection named \"handbook.pdf\" in the database. You can ask me questions related to this collection. What would you like to know?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "your input:  tell me who is ceo and what is company name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n",
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n",
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n",
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n",
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n",
      "searching answers...\n",
      "searching answers...\n",
      "\n",
      "Generated JSON Output:\n",
      "{\n",
      "  \"Who is the CEO?\": \"Shruti Gupta\",\n",
      "  \"What is the company name?\": \"Zania, Inc.\"\n",
      "}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse: Context lenght exceeds. Quiting the chat. Please start fresh!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_bot(chat_history)\n",
      "Cell \u001b[0;32mIn[88], line 57\u001b[0m, in \u001b[0;36mchat_bot\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     55\u001b[0m             assistant_msg \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(output_json, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)}]\n\u001b[1;32m     56\u001b[0m             chat_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m assistant_msg\n\u001b[0;32m---> 57\u001b[0m             chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_bot(chat_history)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# assistant_msg\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_history\n",
      "Cell \u001b[0;32mIn[88], line 57\u001b[0m, in \u001b[0;36mchat_bot\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     55\u001b[0m             assistant_msg \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(output_json, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)}]\n\u001b[1;32m     56\u001b[0m             chat_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m assistant_msg\n\u001b[0;32m---> 57\u001b[0m             chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_bot(chat_history)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# assistant_msg\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_history\n",
      "    \u001b[0;31m[... skipping similar frames: chat_bot at line 57 (3 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[88], line 57\u001b[0m, in \u001b[0;36mchat_bot\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     55\u001b[0m             assistant_msg \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(output_json, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)}]\n\u001b[1;32m     56\u001b[0m             chat_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m assistant_msg\n\u001b[0;32m---> 57\u001b[0m             chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_bot(chat_history)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# assistant_msg\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_history\n",
      "Cell \u001b[0;32mIn[88], line 4\u001b[0m, in \u001b[0;36mchat_bot\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_bot\u001b[39m(chat_history):\n\u001b[0;32m----> 4\u001b[0m     chat_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_request(chat_history, tools\u001b[38;5;241m=\u001b[39mtools, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     output_json \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(chat_answer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m chat_answer\u001b[38;5;241m.\u001b[39mchoices:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py:189\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    188\u001b[0m async_wrapped\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py:111\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/asyncio/__init__.py:114\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 14\u001b[0m, in \u001b[0;36mchat_request\u001b[0;34m(messages, tools, model, stream)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: messages,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1500\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m: stream,\n\u001b[1;32m     13\u001b[0m     }\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_response(params)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating response with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m AsyncClient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m      6\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse generated successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py:1720\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1718\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1719\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1722\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1723\u001b[0m             {\n\u001b[1;32m   1724\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1725\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1726\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1727\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1728\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1729\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1736\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1737\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1738\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1739\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1740\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1741\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1742\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1743\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1754\u001b[0m             },\n\u001b[1;32m   1755\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1756\u001b[0m         ),\n\u001b[1;32m   1757\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1758\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1759\u001b[0m         ),\n\u001b[1;32m   1760\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1761\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1762\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1763\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1846\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py:1582\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1583\u001b[0m         request,\n\u001b[1;32m   1584\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1586\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1588\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1654\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1733\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    399\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    400\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    401\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    402\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    403\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py:136\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py:106\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py:177\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py:198\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py:140\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mpending:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 140\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_bio\u001b[38;5;241m.\u001b[39mwrite_eof()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py:1103\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1101\u001b[0m ):\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1103\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/locks.py:214\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "system_msg = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "chat_history = system_msg\n",
    "\n",
    "while True:\n",
    "    user_input = input('your input: ',).strip()\n",
    "    if user_input.lower() == 'q':\n",
    "        print('Response: Goodbye!!')\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        user_msg = [{\"role\": \"user\", \"content\": user_input}]\n",
    "        chat_history = chat_history+user_msg\n",
    "        tokens = count_tokens(json.dumps(chat_history))\n",
    "        if tokens > 100000:\n",
    "            print('Response: Context lenght exceeds. Quiting the chat. Please start fresh!!')\n",
    "            break\n",
    "        chat_history = await chat_bot(chat_history)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "925a176e-ba25-4554-bf32-cea549378c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = input('your input: ',).strip()\n",
    "# print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c6dd0ea-dd64-4b54-ae2a-c228a432e88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nYour task is to answer user questions. The user can ask a single question or multiple questions.\\nYou have access to the below tools to return answers based on user single/multiple questions.\\n\\n# Tool\\n\"name\": \"get_collections\",\\n\"description\": \"Use this function to check exsiting collection in database\",\\n\"name\": \"create_pdf_collection\",\\n\"description\": \"Use this function to create database collection from pdf file path\",\\n\"name\": \"get_answer\",\\n\"description\": \"Use this function to get answers based on context documents from database to user questions. The documents are purely based on user db.\",\\n                     \\n# Flow of User interaction\\ngreet the user in first interaction.\\nand introduce yourself --> you can only provide answers to question based on collection exists in database and list the collections name\\nusing get_collection tool\\n\\nif user collection does not exist ask user to provide pdf path and create collection based on pdf path using tool create_pdf_collection\\npdf path name will be collection name to use. collection creation takes few seconds to ask user to wait few seconds.\\n\\nif user provide collection name , that collection name will be used to answer user query using get_answer tool.\\nyou can call get_answer tool multiple time for multiple questions\\n\\nif user chat_history available then collection name based on latest chats will be used to answer query\\n\\nYOUR ONLY TASK TO ANSWER USER QUERY BASED ON COLLECTION AVAILABLE ON DATABASE.\\n'},\n",
       " {'role': 'user', 'content': 'hi'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Hello! I'm here to help you with your questions. I can provide answers based on specific collections in our database. Let me check the existing collections for you. Please hold on for a moment.\"},\n",
       " {'role': 'assistant', 'content': \"database collection list ['handbook.pdf']\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I found an existing collection named \"handbook.pdf\" in the database. You can ask me questions related to this collection. What would you like to know?'},\n",
       " {'role': 'user', 'content': 'tell me whatis compnay name and who is ceo'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\\n  \"What is the company name?\": \"Zania, Inc.\",\\n  \"Who is the CEO?\": \"Shruti Gupta\"\\n}'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f08ec081-aec6-483f-be80-06b90a46876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_hp7JIZ7drW2c42lN54F4VLkD', function=Function(arguments='{}', name='get_collections'), type='function')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_answer.choices[0].message.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071fe93-c397-496c-bc74-24b924c15e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = 'hi'\n",
    "# chat_history = []\n",
    "# system_msg = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "# user_msg = [{\"role\": \"user\", \"content\": user_query}]\n",
    "# feed_msg = system_msg + user_msg\n",
    "# chat_history += feed_msg \n",
    "# chat_answer = await chat_request(feed_msg, tools=tools, stream=False)\n",
    "# output_json = {}\n",
    "\n",
    "# if hasattr(chat_answer, 'choices') and chat_answer.choices:\n",
    "#     message = chat_answer.choices[0].message\n",
    "    \n",
    "#     if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "#         for call in message.tool_calls:\n",
    "#             if call.function.name == 'get_answer':  # Corrected dot notation\n",
    "#                 arguments = json.loads(call.function.arguments)\n",
    "#                 query = arguments['query']\n",
    "#                 ans,_ = await get_answer(query)\n",
    "#                 output_json[query] = ans['answer']\n",
    "        \n",
    "#         if output_json:\n",
    "#             print(\"\\nGenerated JSON Output:\")\n",
    "#             print(json.dumps(output_json, indent=2))\n",
    "#     elif hasattr(message, 'content') and message.content:\n",
    "#         print(f\"\\nResponse: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea82b94-e70b-4686-af1e-ffe4a20a74ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "\n",
    "    try:\n",
    "        # Get the tokenizer for the specified model\n",
    "        tokenizer = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Default to a generic encoding if the model is unknown\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Tokenize the text and return the token count\n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    return token_count\n",
    "\n",
    "text = \"This is a test sentence to calculate token count for GPT models.\"\n",
    "\n",
    "tokens = count_tokens(json.dumps(chat_history))\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cda993a-3f07-4343-929c-67b0d1c88f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c3f554e-a7bc-4592-a2d0-0eba0c432835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f224023-23d8-40d2-821a-4337a4f9c9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
